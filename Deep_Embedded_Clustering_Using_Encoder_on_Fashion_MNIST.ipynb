{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a539b7",
      "metadata": {
        "id": "77a539b7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c05eed56",
      "metadata": {
        "id": "c05eed56"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "\n",
        "all_labels = np.array(train_data.targets)\n",
        "all_images = train_data.data.unsqueeze(1).float() / 255.0\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "981d714c",
      "metadata": {
        "id": "981d714c"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim=64):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*7*7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, embedding_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return self.fc(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "31937074",
      "metadata": {
        "id": "31937074",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b77d06fe-b6a7-42f4-8e21-1ea25a1ea8d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Classification Loss: 0.5004\n",
            "Epoch 2, Classification Loss: 0.3077\n",
            "Epoch 3, Classification Loss: 0.2617\n",
            "Epoch 4, Classification Loss: 0.2311\n",
            "Epoch 5, Classification Loss: 0.2037\n",
            "Epoch 6, Classification Loss: 0.1813\n",
            "Epoch 7, Classification Loss: 0.1642\n",
            "Epoch 8, Classification Loss: 0.1438\n",
            "Epoch 9, Classification Loss: 0.1298\n",
            "Epoch 10, Classification Loss: 0.1130\n",
            "Epoch 11, Classification Loss: 0.0960\n",
            "Epoch 12, Classification Loss: 0.0824\n",
            "Epoch 13, Classification Loss: 0.0731\n",
            "Epoch 14, Classification Loss: 0.0612\n",
            "Epoch 15, Classification Loss: 0.0548\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "encoder = CNNEncoder(embedding_dim=64).to(device)\n",
        "clf_head = nn.Linear(64, 10).to(device)\n",
        "\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(clf_head.parameters()), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(15):\n",
        "    total_loss = 0\n",
        "    encoder.train()\n",
        "    clf_head.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        feats = encoder(images)\n",
        "        logits = clf_head(feats)\n",
        "        loss = criterion(logits, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Classification Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9cad148",
      "metadata": {
        "id": "a9cad148"
      },
      "outputs": [],
      "source": [
        "\n",
        "encoder.eval()\n",
        "with torch.no_grad():\n",
        "    feats = encoder(all_images.to(device)).cpu().numpy()\n",
        "labels = all_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe30f262",
      "metadata": {
        "id": "fe30f262"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DEC(nn.Module):\n",
        "    def __init__(self, embedding_dim=64, n_clusters=10):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_clusters = n_clusters\n",
        "        self.cluster_centers = nn.Parameter(torch.randn(n_clusters, embedding_dim))\n",
        "        torch.nn.init.xavier_uniform_(self.cluster_centers.data)\n",
        "\n",
        "    def forward(self, z):\n",
        "        q = 1.0 / (1.0 + torch.sum((z.unsqueeze(1) - self.cluster_centers)**2, dim=2))\n",
        "        q = q ** ((1 + 1.0) / 2.0)\n",
        "        q = (q.t() / torch.sum(q, dim=1)).t()\n",
        "        return z, q\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6220979d",
      "metadata": {
        "id": "6220979d"
      },
      "outputs": [],
      "source": [
        "\n",
        "features_tensor = torch.tensor(feats, dtype=torch.float32).to(device)\n",
        "dec_model = DEC(embedding_dim=64, n_clusters=10).to(device)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, n_init=20)\n",
        "preds_init = kmeans.fit_predict(feats)\n",
        "dec_model.cluster_centers.data.copy_(\n",
        "    torch.tensor(kmeans.cluster_centers_, dtype=torch.float32).to(device)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99f5c0da",
      "metadata": {
        "id": "99f5c0da"
      },
      "outputs": [],
      "source": [
        "\n",
        "def target_distribution(q):\n",
        "    weight = q ** 2 / torch.sum(q, dim=0)\n",
        "    return (weight.t() / torch.sum(weight, dim=1)).t()\n",
        "\n",
        "optimizer = optim.Adam(dec_model.parameters(), lr=1e-3)\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "for epoch in range(30):\n",
        "    dec_model.train()\n",
        "    _, q = dec_model(features_tensor)\n",
        "    p = target_distribution(q).detach()\n",
        "    loss = criterion((q + 1e-10).log(), p)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, KL Loss: {loss.item():.6f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dec_model.eval()\n",
        "with torch.no_grad():\n",
        "    z_final, q_final = dec_model(features_tensor)\n",
        "    pred_labels = q_final.cpu().numpy().argmax(axis=1)\n",
        "\n",
        "conf_matrix = confusion_matrix(labels, pred_labels)\n",
        "row_ind, col_ind = linear_sum_assignment(-conf_matrix)\n",
        "mapped_preds = np.zeros_like(pred_labels)\n",
        "for i in range(len(row_ind)):\n",
        "    mapped_preds[pred_labels == col_ind[i]] = row_ind[i]\n",
        "\n",
        "accuracy = np.mean(mapped_preds == labels)\n",
        "print(f\"Clustering Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Silhouette:\", silhouette_score(z_final.cpu(), pred_labels))\n",
        "print(\"CH Index:\", calinski_harabasz_score(z_final.cpu(), pred_labels))\n",
        "print(\"DB Index:\", davies_bouldin_score(z_final.cpu(), pred_labels))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IegcupQdILKa"
      },
      "id": "IegcupQdILKa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sample_indices = np.random.choice(len(z_final), size=3000, replace=False)\n",
        "z_sample = z_final[sample_indices].cpu().numpy()\n",
        "label_sample = mapped_preds[sample_indices]\n",
        "\n",
        "z_tsne = TSNE(n_components=2, n_iter=500, random_state=42).fit_transform(z_sample)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(z_tsne[:, 0], z_tsne[:, 1], c=label_sample, cmap='tab10', s=10)\n",
        "plt.title(\"t-SNE Clustering Visualization\")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "SS8lKhqlIN6j"
      },
      "id": "SS8lKhqlIN6j",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}